{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating a Linear Model\n",
    "*Curtis Miller*\n",
    "\n",
    "MSE is a useful metric for seeing the performance of a model, but other metrics can help us decide which model to use.\n",
    "\n",
    "Here we will use **statsmodels** for fitting linear models since the package easily computes the metrics I want to see.\n",
    "\n",
    "Let's load in the Boston housing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "boston_obj = load_boston()\n",
    "data_train, data_test, price_train, price_test = train_test_split(boston_obj.data, boston_obj.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting a Linear Model with OLS in **statsmodels**\n",
    "\n",
    "The `OLS` object in **statsmodels** handles fitting models with OLS. Below I show how to fit the same model for Boston home prices I fitted using **scikit-learn**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test = sm.add_constant(data_train), sm.add_constant(data_test)    # Necessary to add the intercept\n",
    "data_train[:5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train[:5, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols1 = sm.OLS(price_train, data_train)    # Target, features\n",
    "model1 = ols1.fit()\n",
    "model1.params    # The parameters of the regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.predict([[    # An example prediction\n",
    "    1,      # Intercept term; always 1\n",
    "    10,     # Per capita crime rate\n",
    "    25,     # Proportion of land zoned for large homes\n",
    "    5,      # Proportion of land zoned for non-retail business\n",
    "    1,      # Tract bounds the Charles River\n",
    "    0.3,    # NOX concentration\n",
    "    10,     # Average number of rooms per dwelling\n",
    "    2,      # Proportion of owner-occupied units built prior to 1940\n",
    "    10,     # Weighted distance to employment centers\n",
    "    3,      # Index for highway accessibility\n",
    "    400,    # Tax rate\n",
    "    15,     # Pupil/teacher ratio\n",
    "    200,    # Index for number of blacks\n",
    "    5       # % lower status of population\n",
    "]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get a summary of the model easily in **statsmodels**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's parse these results:\n",
    "\n",
    "* `R-squared` (mathematically, $R^2$) describes how much variation in the target variable the model is able to \"explain.\" (Think: predict.) Practitioners prefer `Adj. R-squared` ($\\bar{R}^2$) since this takes into account how many variables are used. (it is impossible for $R^2$ to go down when adding variables even if those variables only contribute noise; $\\bar{R}^2$ doesn't have this property.) Here $\\bar{R}^2$ is somewhat high, suggesting that the model does a reasonable job at predicting home prices.\n",
    "* `F-statistic` is the test statistic for a statistical test to determine if any coefficients in the model are not zero. `Prob (F-statistic)` is the corresponding $p$-value. Here the model clearly has a non-zero coefficient, though the statistic does not say which.\n",
    "* `AIC` is the **Akaike information criterion (AIC)**, and `BIC` the **Bayesian information criterion (BIC)**. These are measures of the quality of statistical models and provide a means to decide between models. Models that lead to smaller AIC and BIC are seen as better.\n",
    "* The table contains the coefficients of the statistical model and the results of $t$-tests to determine if the coefficients are zero or not, in addition to confidence intervals for the coefficient values. We might consider removing features with coefficients not statistically different from zero from our model (but we should also refer to the AIC and BIC statistics when making decisions between models).\n",
    "\n",
    "## Using AIC to Pick Models\n",
    "\n",
    "Let's see how we can use the AIC to decide between different models. (The BIC can be used similarly.) Notice that in our table the third and seventh features don't have coefficients statistically different from zero (these correspond to proportion of non-retail business acres per town and proportion of owner-occupied units built prior to 1940). Does a model without these features do better according to the AIC?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols2 = sm.OLS(price_train, np.delete(data_train, [3, 7], axis=1))\n",
    "model2 = ols2.fit()\n",
    "print(model2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model has a smaller AIC.\n",
    "\n",
    "The different AIC values can be interpreted this way: If model 1 has $\\text{AIC}_1$ and model 2 $\\text{AIC}_2$ and $\\text{AIC}_2 < \\text{AIC}_1$, then model 1 is $\\exp((\\text{AIC}_2 - \\text{AIC}_1)/2)$ times more likely to minimize information loss than model 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp((model2.aic - model1.aic)/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inverse of that quantity can be interpreted as how much more likely model 2 is to minimize the information loss than model 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp((model1.aic - model2.aic)/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that model 2 should be preferred to model 1.\n",
    "\n",
    "Let's see how model 2 would have done on the test set, evaluating its performance with the MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_train_pred = model2.predict(np.delete(data_train, [3, 7], axis=1))\n",
    "mean_squared_error(price_train, price_train_pred)     # Performance on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_test_pred = model2.predict(np.delete(data_test, [3, 7], axis=1))\n",
    "mean_squared_error(price_test, price_test_pred)     # Performance on the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In comparison to model 1..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_test_pred_mod1 = model1.predict(data_test)\n",
    "mean_squared_error(price_test, price_test_pred_mod1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 2 did better."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
