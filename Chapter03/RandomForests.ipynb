{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Using Random Forests\n",
    "*Curtis Miller*\n",
    "\n",
    "A **random forest** is a collection of decision trees that each individually make a prediction for an observation. Each tree is formed from a random subset of the training set. The majority decision among the trees is then the predicted value of an observation. Random forests are an example of **ensemble methods**, where the predictions of individual classifiers are used for decision making.\n",
    "\n",
    "The **scikit-learn** class `RandomForestClassifier` can be used for training random forests. For random forests we may consider an additional hyperparameter to tree depth: the number of trees to train. Each tree should individually be shallow, and having more trees should lead to less overfitting.\n",
    "\n",
    "We will still be using the *Titanic* dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.metrics import classification_report\n",
    "from random import seed    # Set random seed for reproducible results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed(110717)    # Set the seed\n",
    "titanic = pd.read_csv(\"titanic.csv\")\n",
    "titanic_train, titanic_test = train_test_split(titanic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Growing a Random Forest\n",
    "\n",
    "Let's generate a random forest where I cap the depth for each tree at $m = 5$ and grow 10 trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest1 = RandomForestClassifier(n_estimators=10,    # Number of trees to grow\n",
    "                                 max_depth=5)        # Maximum depth of a tree\n",
    "forest1.fit(X=titanic_train.replace({'Sex': {'male': 0, 'female': 1}}    # Replace strings with numbers\n",
    "                                   ).drop([\"Survived\", \"Name\"], axis=1),\n",
    "            y=titanic_train.Survived)\n",
    "\n",
    "# Example prediction\n",
    "forest1.predict([[2, 0, 26, 0, 0, 30]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = forest1.predict(titanic_train.replace({'Sex': {'male': 0, 'female': 1}}\n",
    "                                             ).drop([\"Survived\", \"Name\"], axis=1))\n",
    "print(classification_report(titanic_train.Survived, pred1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest does not perform as well on the training data as a full-grown decision tree, but such a tree overfit. The random forest, in comparison, seems to do as well as a better decision tree so far.\n",
    "\n",
    "## Optimizing Multiple Hyperparameters\n",
    "\n",
    "We now have two hyperparameters to optimize: tree depth and the number of trees to grow. We have a few ways to proceed:\n",
    "\n",
    "1. We could use cross-validation to see which combination of hyperparameters performs the best. Beware that there could be many combinations to check!\n",
    "2. We could use cross-validation to optimize one hyperparameter first, then the next, and so on. While not necessarily producing a globally optimal solution this is less work and likely yields a \"good enough\" result.\n",
    "3. We could randomly pick combinations of hyperparameters and use the results to guess a good combination. This is like 1 but less work.\n",
    "\n",
    "Here I will go with option 2. I will optimize the number of trees to use first, then maximum tree depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_candidate = [10, 20, 30, 40, 60, 80, 100]    # Candidate forest sizes\n",
    "res1 = dict()\n",
    "\n",
    "for n in n_candidate:\n",
    "    pred3 = RandomForestClassifier(n_estimators=n, max_depth=5)\n",
    "    res1[n] = cross_validate(pred3,\n",
    "                            X=titanic_train.replace({'Sex': {'male': 0, 'female': 1}}    # Replace strings with numbers\n",
    "                                         ).drop([\"Survived\", \"Name\"], axis=1),\n",
    "                            y=titanic_train.Survived,\n",
    "                            cv=10,\n",
    "                            return_train_score=False,\n",
    "                            scoring='accuracy')\n",
    "\n",
    "res1df = DataFrame({(i, j): res1[i][j]\n",
    "                             for i in res1.keys()\n",
    "                             for j in res1[i].keys()}).T\n",
    "\n",
    "res1df.loc[(slice(None), 'test_score'), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res1df.loc[(slice(None), 'test_score'), :].mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$n = 100$ seems to do well. Now let's pick optimal tree depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m_candidate = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]    # Candidate depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res2 = dict()\n",
    "\n",
    "for m in m_candidate:\n",
    "    pred3 = RandomForestClassifier(max_depth=m, n_estimators=40)\n",
    "    res2[m] = cross_validate(pred3,\n",
    "                             X=titanic_train.replace({'Sex': {'male': 0, 'female': 1}}    # Replace strings with numbers\n",
    "                                          ).drop([\"Survived\", \"Name\"], axis=1),\n",
    "                             y=titanic_train.Survived,\n",
    "                             cv=10,\n",
    "                             return_train_score=False,\n",
    "                             scoring='accuracy')\n",
    "\n",
    "res2df = DataFrame({(i, j): res2[i][j]\n",
    "                             for i in res2.keys()\n",
    "                             for j in res2[i].keys()}).T\n",
    "\n",
    "res2df.loc[(slice(None), 'test_score'), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res2df.loc[(slice(None), 'test_score'), :].mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A maximum tree depth of $m = 7$ seems to work well. A way to try and combat the path-dependence of this approach would be to repeat the search for optimal forest size but with the new tree depth and so on, but I will not do so here.\n",
    "\n",
    "Let's now see how the new random forest performs on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forest2 = RandomForestClassifier(max_depth=9, n_estimators=40)\n",
    "forest2.fit(X=titanic_train.replace({'Sex': {'male': 0, 'female': 1}}    # Replace strings with numbers\n",
    "                                   ).drop([\"Survived\", \"Name\"], axis=1),\n",
    "            y=titanic_train.Survived)\n",
    "\n",
    "survived_test_predict = forest2.predict(X=titanic_test.replace(\n",
    "    {'Sex': {'male': 0, 'female': 1}}\n",
    ").drop([\"Survived\", \"Name\"], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(titanic_test.Survived, survived_test_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest does reasonably well, though it does not appear to be much of an improvement over the decision tree. Given the complexity of the random forest, a simple decision tree would be preferred."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
